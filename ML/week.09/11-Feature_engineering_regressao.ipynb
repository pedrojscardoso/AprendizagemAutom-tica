{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "___\n",
    "<h1> Machine Learning </h1>\n",
    "<h2> Systems Engineering and Computer Technologies / Engenharia de Sistemas e Tecnologias Inform√°ticas\n",
    "(LESTI)</h2>\n",
    "<h3> Instituto Superior de Engenharia / Universidade do Algarve </h3>\n",
    "\n",
    "[LESTI](https://ise.ualg.pt/curso/1941) / [ISE](https://ise.ualg.pt) / [UAlg](https://www.ualg.pt)\n",
    "\n",
    "Pedro J. S. Cardoso (pcardoso@ualg.pt)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Feature engineering\n",
    "\n",
    "Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself. Examples of feature engineering include:\n",
    "- deriving new features from existing data,\n",
    "- selecting only the most relevant features,\n",
    "- creating features from images, text, and sensor data,\n",
    "- normalizing numerical features,\n",
    "- encoding categorical features,\n",
    "- transforming features into a more suitable format for machine learning algorithms.\n",
    "- ...\n",
    "\n",
    "In this notebook, we will explore some of these techniques. For that, let us consider the Seoul Bike Sharing Demand dataset. The dataset contains the hourly count of rental bikes between years 2017 and 2018 in Seoul, Korea with the corresponding weather and seasonal information. The dataset can be downloaded from https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand but we have already downloaded it and saved it in the `data` folder.\n",
    "\n",
    "So, we can start by loading the dataset into a pandas dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:20:41.553573Z",
     "start_time": "2024-11-10T23:20:41.536443Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./../../Datasets/SeoulBikeData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "By calling the dataframe's `info` method, we can see that there are no missing values but there are some categorical columns.\n",
    "(For treating missing values, please refer to the `12-Missing-Data.ipynb` notebook were some techiniques are studied.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:20:42.339820Z",
     "start_time": "2024-11-10T23:20:42.335172Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Categorical data transformation\n",
    "\n",
    "Most machine learning algorithms cannot handle categorical data. Therefore, categorical data must be transformed into numerical data. There are several ways to do this, like:\n",
    "- One-hot encoding -- transform each category into a binary column\n",
    "- Ordinal encoding -- transform each category into a number\n",
    "- Binary encoding -- transform each category into a binary number\n",
    "- Hash encoding -- transform each category into a hash number\n",
    "- ...\n",
    "\n",
    "Let us see how to performe the first two techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### One hot encoding\n",
    "\n",
    "One hot encoding is a technique used to transform categorical features to binary features. The idea is to create a new column for each category and assign a 1 or 0 to the column. For example, the season column has four categories: Spring, Summer, Autumn, and Winter. We can convert this column into 3 columns and use 0/False or 1/True to indicate if the sample belongs to that category or not. We only need 3 columns because if the sample is not in the first three categories, then it must be in the fourth category.\n",
    "\n",
    "\n",
    "To achieve this, we can use the pandas get_dummies method (we'll do it for the 'Holiday' and 'Functioning Day' columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:20:44.447195Z",
     "start_time": "2024-11-10T23:20:44.438687Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['Holiday', 'Functioning Day'], drop_first=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Ordinal encoding\n",
    "Ordinal encoding is a technique used to transform categorical features to ordinal features. The idea is to assign a number to each category. For example, the season column has four categories: Spring, Summer, Autumn, and Winter. We can convert this column into a single column with values 1, 2, 3, and 4. To achieve this, we can use the pandas replace method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:20:47.195159Z",
     "start_time": "2024-11-10T23:20:47.186228Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df['Seasons'] = df['Seasons'].replace({'Spring': 1, 'Summer': 2, 'Autumn': 3, 'Winter': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Dates' transformation\n",
    "\n",
    "Dates are usually represented as strings or a specific data type. However, machine learning algorithms cannot handle strings. Therefore, dates must be transformed into numerical data. There are several ways to do this, like extracting the year, month, day, day of week etc. from the date\n",
    "\n",
    "In our case, we can split this column into two columns: month and day, and day of week. To achieve this, we can use the pandas `to_datetime``\n",
    " method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:20:53.535643Z",
     "start_time": "2024-11-10T23:20:53.521402Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure the date column is in datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "And now, extract the wanted data (month, day etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns for month, day, and day of week\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['day'] = df['Date'].dt.day\n",
    "df['day_of_week'] = df['Date'].dt.day_of_week\n",
    "\n",
    "# drop the original date column\n",
    "df.drop('Date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let us now recheck the dataframe's info method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:20:57.193207Z",
     "start_time": "2024-11-10T23:20:57.185963Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We should be now able to apply machine learning algorithms to this dataset. However, we can still improve the performance of the algorithms by applying some feature engineering techniques. But let us see how the algorithms perform without any feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:21:12.850336Z",
     "start_time": "2024-11-10T23:21:11.218042Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_X_and_y(df):\n",
    "    X = df.drop(['Rented Bike Count'], axis=1)\n",
    "    y = df['Rented Bike Count']\n",
    "    return X, y\n",
    "\n",
    "def run(df):\n",
    "    # get X and y\n",
    "    X, y = get_X_and_y(df)\n",
    "    \n",
    "    # split the data into train and test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, shuffle=True)\n",
    "\n",
    "    models = {\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'Ridge': Ridge(),\n",
    "        'Lasso': Lasso(),\n",
    "        'SVR': NuSVR(),\n",
    "        'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "        'RandomForestRegressor': RandomForestRegressor(),\n",
    "        # 'MLPRegressor': MLPRegressor(max_iter=10000)\n",
    "    }\n",
    " \n",
    "    fig, ax = plt.subplots(len(models), 1, figsize=(10, 40))\n",
    "    scores = {}\n",
    "    for idx, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        pred = model.predict(X_test)\n",
    "        print(f'{name}: score = {score}')\n",
    "\n",
    "        # plot pred vs actual\n",
    "        ax[idx].plot(y_test.values, pred, c='g', marker='o', linestyle='None')\n",
    "        ax[idx].plot(y_test.values, y_test.values, c='r')\n",
    "        ax[idx].set_ylabel('Predicted')\n",
    "        ax[idx].set_xlabel('Actual')\n",
    "        ax[idx].set_title(f'{name} / Score =  {score}')   \n",
    "        \n",
    "        scores[name] = score\n",
    "    \n",
    "    return scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:21:31.794848Z",
     "start_time": "2024-11-10T23:21:14.489582Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_scores = pd.DataFrame()\n",
    "all_scores['without scaling or poly'] = run(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Feature scaling\n",
    "Feature scaling is the process of transforming numerical features to a common scale. There are several ways to do this, like:\n",
    "- Normalization -- transform each feature to a range between 0 and 1\n",
    "- Standardization -- transform each feature to a normal distribution with mean 0 and standard deviation 1\n",
    "- etc.\n",
    "\n",
    "The original dataset has the following distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:21:31.810403Z",
     "start_time": "2024-11-10T23:21:31.796050Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Box plots also help with visualization of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:21:31.930965Z",
     "start_time": "2024-11-10T23:21:31.810946Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['Rented Bike Count'], axis=1).plot(kind='box', figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Standardization (or Z-score normalization)\n",
    "\n",
    "Standardization is a technique used to transform numerical features to a normal distribution with mean 0 and standard deviation 1. The idea is to subtract the mean and divide by the standard deviation. The formula is given by\n",
    "$$ X'_{ij} = \\frac{X_{ij}-\\mu_j}{\\sigma_j}$$\n",
    "where $X_{ij}$ is the observation $i$ for the feature $j$, $\\mu_j$ is the mean and $\\sigma_j$ is the standard deviation.\n",
    "\n",
    "\n",
    "To achieve this, we can use the pandas mean and std methods or call the sklearn StandardScaler method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let us now apply the standardization technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:21:37.715768Z",
     "start_time": "2024-11-10T23:21:37.599373Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# get X and y\n",
    "X, y = get_X_and_y(df)\n",
    "\n",
    "# set and fit the scaler\n",
    "standard_scaler = StandardScaler().fit(X)\n",
    "\n",
    "# normalize the data\n",
    "df_std = pd.DataFrame(standard_scaler.transform(X), columns = X.columns)\n",
    "df_std.plot(kind='box', figsize=(20,5))\n",
    "\n",
    "df_std['Rented Bike Count'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "So, let us create a model but now using the standarderized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:22:18.734867Z",
     "start_time": "2024-11-10T23:21:40.463554Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_scores['with standardization'] =  run(df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### MinMaxScaler\n",
    "\n",
    "Another usual solution is to normalize the distribution by subtracting the minimum and dividing by the difference between the maximum and the minimum,\n",
    "\n",
    "$$ X'_{ij} = \\frac{X_{ij}-\\min_j}{\\max_j-\\min_j}$$\n",
    "\n",
    "where X_{ij} is the observation $i$ for the feature $j$, $\\min_j$ is the minimum and $\\max_j$ is the maximum. Returned values are in the range [0, 1].\n",
    "\n",
    "This can be done by coding or simply using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:22:18.943056Z",
     "start_time": "2024-11-10T23:22:18.736589Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X, y = get_X_and_y(df)\n",
    "\n",
    "# set and fit the scaler\n",
    "minmax_scaler = MinMaxScaler().fit(X)\n",
    "\n",
    "df_minmax = pd.DataFrame(minmax_scaler.transform(X), columns = X.columns)\n",
    "df_minmax.plot(kind='box', figsize=(20,5))\n",
    "\n",
    "df_minmax['Rented Bike Count'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "So, let us create a model but now using the scaled data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T23:23:54.013314Z",
     "start_time": "2024-11-10T23:22:18.944439Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_scores['with minmax'] = run(df_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Polynomial features\n",
    "\n",
    "Other approach is to create polynomial features. In this case, if the original set of feature is $(x_1, x_2, ..., x_n)$ then the polynomial features with degree 2 are $(1, x_1, x_2, x_n, x_1^2, x_1x_2, \\ldots, x_1x_n, x_2^2, \\ldots, x_2x_n, \\ldots,  x_n^2 \\ldots)$.\n",
    "\n",
    "This can be done by coding or simply using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = get_X_and_y(df)\n",
    "\n",
    "# set and fit the scaler\n",
    "poly = PolynomialFeatures(degree=2).fit(X)\n",
    "\n",
    "df_poly = pd.DataFrame(poly.transform(X), columns = poly.get_feature_names_out(X.columns))\n",
    "\n",
    "df_poly.plot(kind='box', figsize=(20,5))\n",
    "\n",
    "df_poly['Rented Bike Count'] = y\n",
    "\n",
    "df_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Train a model using the polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_scores['with poly'] = run(df_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Normalization + Polynomial features\n",
    "\n",
    "Now, let us combine both normalization and polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get X and y\n",
    "X, y = get_X_and_y(df)\n",
    "\n",
    "# set and fit the scaler\n",
    "standard_scaler = StandardScaler().fit(X)\n",
    "\n",
    "# normalize the data\n",
    "df_std = pd.DataFrame(standard_scaler.transform(X), columns = X.columns)\n",
    "\n",
    "# set and fit the scaler\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False).fit(df_std)\n",
    "\n",
    "df_std_poly = pd.DataFrame(poly.transform(df_std), columns = poly.get_feature_names_out(df_std.columns))\n",
    "\n",
    "df_std_poly.plot(kind='box', figsize=(20,5))\n",
    "\n",
    "df_std_poly['Rented Bike Count'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "And run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_scores['with standardization and poly'] =  run(df_std_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_scores.plot(figsize=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
